{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIP Framework training example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `/z1-josemm/josemm/MyDocs/BIPs.jl`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"..\")\n",
    "using BIPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics\n",
    "using Pkg.Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets begin by bringing in the dataset. It contains tree splits:\n",
    "* **train**: the training set with 1M jets\n",
    "* **validation**: the validation set with 400k jets\n",
    "\n",
    "And of course later we will use the **test** set with other 400k jets to report the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"../../DataLake/raw/val.h5\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_path = \"../../DataLake/raw\"\n",
    "# dataset_path = \"/Users/ortner/datasets/toptagging\"\n",
    "\n",
    "train_data_path = dataset_path*\"/train.h5\"\n",
    "val_data_path = dataset_path*\"/val.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data\n",
    "\n",
    "In order to read the datasets, we call the `read_dataset` function:\n",
    "to read the TopQuark format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_jets, train_labels = BIPs.read_data(\"TQ\", train_data_path)\n",
    "train_labels = [reinterpret(Bool, b == 1.0) for b in train_labels]\n",
    "print(\"Number of entries in the training data: \", length(train_jets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_jets, val_labels = BIPs.read_data(\"TQ\", val_data_path)\n",
    "val_labels = [reinterpret(Bool, b == 1.0) for b in val_labels]\n",
    "print(\"Number of entries in the validation data:\", length(val_jets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets examine how one of the jets looks like, each one of the entries is one detected particle's four momentum $(E, p_x, p_y, p_z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However,in order to compute the embeddings, it is necesary to convert the jets to a format that can be used by the framework. The function `data2hyp` allows to convert each detected four momentum to the jet basis, a.k.a $(\\tilde p_T, \\cos(\\theta), \\sin(\\theta), \\tilde y, E_T)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transf_jets = data2hyp(train_jets)\n",
    "val_transf_jets = data2hyp(val_jets)\n",
    "println(\"Transformed jets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the jets are converted to the jet basis, it is moment to embed the model using the *Invariant Polynomials*. \n",
    "\n",
    "The function `build_ip` allocates efficiently the sparse basis, while the `bip_data` computes the invariant representation of each one of the jets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_bip, specs = build_ip(order=3, levels=7)\n",
    "    \n",
    "function bip_data(dataset_jets)\n",
    "    storage = zeros(length(dataset_jets), length(specs))\n",
    "    for i = 1:length(dataset_jets)\n",
    "        storage[i, :] = f_bip(dataset_jets[i])\n",
    "    end\n",
    "    storage[:, 2:end]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedded_jets = bip_data(train_transf_jets)\n",
    "println(\"Embedded train jets correclty\")\n",
    "val_embedded_jets = bip_data(val_transf_jets)\n",
    "println(\"Embedded test jets correclty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings are now created for the dataset. From this point on, the classification itself is absolutelly versatile. For this specific example we will use the out-of-the box classifier `sklearn.linear_model.HistGradientBoostingClassifier` that bines the data and then applies a grandient boosted trees algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets fit a simple model to the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyCall\n",
    "@pyimport sklearn.ensemble as sk_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCT = sk_ensemble.HistGradientBoostingClassifier(verbose=true).fit(train_embedded_jets, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "Plots.plot(GCT[:validation_score_], label=\"Validation\", xlabel=\"Iteration\", ylabel=\"Log Loss\")\n",
    "Plots.plot!(GCT[:train_score_], label=\"Training\", title=\" Trainig curve for the Histogram GB Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lest test how we do performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understanad the framework, lets see how our model performs on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = \"../../DataLake/raw/test.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_jets, test_labels = BIPs.read_data(\"TQ\", test_data_path)\n",
    "test_labels = [reinterpret(Bool, b == 1.0) for b in test_labels]\n",
    "test_transf_jets = data2hyp(test_jets)\n",
    "test_embedded_jets = bip_data(test_transf_jets)\n",
    "print(\"Embedded test jets correclty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = GCT.score(test_embedded_jets, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probas = GCT.predict_proba(test_embedded_jets)\n",
    "bkg_index = [label==false for label in test_labels]\n",
    "Plots.histogram(test_probas[:, 2][test_labels], color=\"Blue\", label=\"Top Jets\", xlabel=\"Model's Output Probability\", ylabel=\"Number of Jets\")\n",
    "Plots.histogram!(test_probas[:, 2][bkg_index], color=\"Red\", label=\"QCD Jets\", title=\"Probability Scores\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f075184c5ef2f0b32d809a510d5efd6bb855cd1f11e474e44795b0201961dbbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
