{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIP Framework training example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"..\")\n",
    "using BIPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics\n",
    "using Pkg.Artifacts\n",
    "using DelimitedFiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets begin by bringing in the dataset. It contains tree splits:\n",
    "* **train**: the training set with 1M jets\n",
    "* **validation**: the validation set with 400k jets\n",
    "\n",
    "And of course later we will use the **test** set with other 400k jets to report the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../../../DataLake/raw\"\n",
    "# dataset_path = \"/Users/ortner/datasets/toptagging\"\n",
    "\n",
    "train_data_path = dataset_path*\"/train.h5\"\n",
    "val_data_path = dataset_path*\"/val.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data\n",
    "\n",
    "In order to read the datasets, we call the `read_dataset` function:\n",
    "to read the TopQuark format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_jets, train_labels = BIPs.read_data(\"TQ\", train_data_path)\n",
    "train_labels = [reinterpret(Bool, b == 1.0) for b in train_labels]\n",
    "print(\"Number of entries in the training data: \", length(train_jets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_jets, val_labels = BIPs.read_data(\"TQ\", val_data_path)\n",
    "val_labels = [reinterpret(Bool, b == 1.0) for b in val_labels]\n",
    "print(\"Number of entries in the validation data: \", length(val_jets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets examine how one of the jets looks like, each one of the entries is one detected particle's four momentum $(E, p_x, p_y, p_z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However,in order to compute the embeddings, it is necesary to convert the jets to a format that can be used by the framework. The function `data2hyp` allows to convert each detected four momentum to the jet basis, a.k.a $(\\tilde p_T, \\cos(\\theta), \\sin(\\theta), \\tilde y, E_T)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transf_jets = data2hyp(train_jets)\n",
    "val_transf_jets = data2hyp(val_jets)\n",
    "println(\"Transformed jets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the jets are converted to the jet basis, it is moment to embed the model using the *Invariant Polynomials*. \n",
    "\n",
    "The function `build_ip` allocates efficiently the sparse basis, while the `bip_data` computes the invariant representation of each one of the jets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_bip, specs = build_ip(order=4, levels=8)\n",
    "    \n",
    "function bip_data(dataset_jets)\n",
    "    storage = zeros(length(dataset_jets), length(specs))\n",
    "    for i = 1:length(dataset_jets)\n",
    "        storage[i, :] = f_bip(dataset_jets[i])\n",
    "    end\n",
    "    storage[:, 2:end]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedded_jets = bip_data(train_transf_jets)\n",
    "println(\"Embedded train jets correclty\")\n",
    "val_embedded_jets = bip_data(val_transf_jets)\n",
    "println(\"Embedded test jets correclty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = \"../../../DataLake/raw/test.h5\"\n",
    "test_jets, test_labels = BIPs.read_data(\"TQ\", test_data_path)\n",
    "test_labels = [reinterpret(Bool, b == 1.0) for b in test_labels]\n",
    "test_transf_jets = data2hyp(test_jets)\n",
    "test_embedded_jets = bip_data(test_transf_jets)\n",
    "print(\"Embedded test jets correclty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale(A) = (A .- mean(A, dims=1)) ./ std(A, dims=1)\n",
    "\n",
    "##\n",
    "X_train, y_train = scale(train_embedded_jets), train_labels\n",
    "X_test, y_test = scale(test_embedded_jets), test_labels\n",
    "X_val, y_val = scale(val_embedded_jets), val_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFIERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyCall\n",
    "@pyimport sklearn.neural_network as sk_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = sk_nn.MLPClassifier(verbose=true, max_iter=2000, hidden_layer_sizes=(200,100), n_iter_no_change=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.score(X_val, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_test_scores = mlp.predict_proba(X_test)\n",
    "writedlm( \"../foo/mlp_test_probas2.csv\",  mlp_test_scores, ',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using XGBoost\n",
    "function fit_xgb_clf(X_train, y_train, X_val, y_val)\n",
    "    dtrain = DMatrix(X_train, label=y_train)\n",
    "    dtest = DMatrix(X_val, label=y_val)\n",
    "    param = [\"silent\" => 0, \"subsample\" => 0.5, \"colsample_bytree\" => 0.5, \"eta\" => 0.05]\n",
    "    watchlist = [(dtest, \"test\"), (dtrain, \"train\")]\n",
    "    bst = xgboost(X_train, 500, label=y_train, param=param, objective = \"binary:logistic\",\n",
    "        watchlist=watchlist, metrics=[\"logloss\", \"auc\", \"error\"],\n",
    "        early_stopping_rounds=50, verbose_eval=10, seed=137)\n",
    "    \n",
    "    bst\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fit_xgb_clf(X_train, y_train, X_val, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_probas = predict(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writedlm( \"../foo/xgb_test_probas2.csv\",  xgb_probas, ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f075184c5ef2f0b32d809a510d5efd6bb855cd1f11e474e44795b0201961dbbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
